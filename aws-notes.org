
;; Fundamentals
* Need to know
** EC2
*** how to ssh into ec2 
*** how to change .pem permissions (chmod)
gives a permission error exception if you can't access .pem
*** how to use security groups
*** differences between private/public/elastic IP
*** how to use User Data at boot time
*** know that you can use custom AMI
*** instances are billed per second
* Linux commands
** install
yum install
** switch to root
sudo su
* Regions
** All regions are scoped
Except for IAM and s3
* AMI
Amazon Machine Image
They are region locked, cannot be seen from outside a region
* IAM
Identity and Access Management
It's global ( not locked to a zone / region )
* SECURITY
Never use root account ( except when first setting up )
*** Users
Physical person
*** Groups
Functions ( admins )
*** Roles
For machines
*** Policies
Govern permissions
Define what each above groups can do
*** Leas privilege principle 
Give minimal permissions to users they need to get job done
*** Permissions
**** 0644 <key_name> are too open
chmod 0400 <key_name> (changes permissions to -r-----) (only owner can read)
*** Security groups
Act like firewall and block all Inboud / Outbound traffic that is not greenlit
Can be attached to multiple instances ( vm's )
Instance can have mutliple groups
They work only within their region ( if you switch region, you have to recreate groups )
They 'live' outside EC2 - they don't run on instances
Defaults: all inbound is blocked and all outbound is allowed
You can reference other security groups / CIDR blocks / IP addresses
You CANNOT reference DNS names 
* EC2
Elastic compute cloud
** Cannot see IP of customer going through load balancer
Can only see private IP of ALB, in order to get IP of client
check the header 'X-Forwarded-For'
to check the port check 'X-Forwarded-Port'
** Custom AMI
Customers can spin up their own versions of OS
with custom setup/software/access etc
** T2 Instances 
They are burstable :
they have 'cpu credits', and under unexpected load
they can perform very well, unless they run out of cpu creds
** User Data
Used to run commands when the instance starts up
Commands run with 'sudo'
Every bash script has to start with
#+BEGIN_CENTER bash
   #!/bin/bash
#+END_CENTER
** shh into machine
ssh -i <key-file.pem> ec2-user@ip-address
** shh on windows
use PUTTY to ssh into it
** Launch Types
*** On demand
short workload, predictable pricing
*** Reserved Instances
long workloads ( 1+ year )
*** Convertible Reserved Instances
long workloads with flexi instances
*** Scheduled Reserved Instances
launch within reserved time window
*** Spot Instances
short workloads, cheap, can loose instances!
*** Dedicated Instances
no other customer will share hardware
*** Dedicated Hosts
book entire server
** Pricing
Depends on:
-region
-instance type
-on demand / reserved / dedicated / spot
-type of OS
-billed per second, with min of 60
* Apache
** install apache
yum install httpd.x86_64
(http daemon)
** run & enable apache
systemctl start httpd.service
systemctl enable httpd.service
* Load Balancers
** Can scale but need to 'warm up'
** CLB/ALB/NLB has a static host name 
Do not resolve and use underlying IP
** Provide SSL certs & SSL termination (Classic and ALB)
** Types
v1 balancer ( classic )
v2 application / network balancer
** Health checks ( available to all ALB )
ELB can perform health check on instance
Done on PORT & ROUTE ( /health )
** Application Load Balancer (v2)
Layer 7 
Can route based on hostname / path
Great fit with ECS (Docker)
Can handle/serve multiple groups/apps
Can implement 'stickiness' and direct same user to the same group ( ALB will generate cookie, not your application !)
Supports HTTP/HTTPS & Websockets protocols
APPLICATIONS dont see IP of clients directly! ( Ip is placed in header 'X-Forwarded-For)
** Network Load Balancer (v2)
Layer 4
Forwards TCP traffic
** CLB/ALB cannot see client IP directly
NLB can see IP directly
Ip is stored in 'X-Forwarded-For' header (for alb/clb)
** 4xx errors
Client induced error
** 5xx errors
Application/Server side errors
** If unable to connect to application..
Check security groups!
* ASG
Auto Scaling Groups
Free!
Can scale based on CloudWatch alarms
Its possible to set up custom metrics for CloudWatch
ASG use launch configurations 
IAM roles attached to ASG are PASSED on to EC2 instances 
** Metrics
CPU
Network
Custom
On schedule!
* EBS Volume
Network drives
Only one EBS can be attached/detached to EC2's
Locked to Availability Zone eg: us-east-1a != us-east-1b
You can move SNAPSHOTS of it
EBS backups use IO so don't perform them when application is busy
Root EBS gets terminated with the instance ( can be turned off )
Get charged by provisioned size, not used one
You can resize volumes
** EBS encryption !important
Data at rest is encrypted inside the volume
Data moving between instance & volume is encrypted
Snapshots created from this volume are encrypted
** Snapshots
Snapshots take actual size not the whole provision of EBS 
Used for backups
When you want to resize a volume down
Change volume type
Encrypt volume
** Instance stores
Physically attached to the machine
Better I/O performance
On termination of instance, data is lost
Cant resize
Backups must be operated by user
* Route 53
Use Alias over CNAME
Managed DNS ( collection of records )
DNS records get cached by browsers ( saved locally )
** Records:
*** A: 
url to ipv4
*** AAAA:
url to ipv6
*** CNAME
url to url
*** Alias
url to AWS resource
** Features:
Load balancing
Health checks
Routing policy ( geolocation, proximity, latency, weighted, simple)
* RDS
Relational Database Services
** Replicas
Applications MUST update the connection string to leverage read replicas!!
Used for READ scaling !
DB can have up to 5 replicas within & across AZ or regions
Replication is ASYNC !
Replicas can be promoted to their own DB
Master has 'write/read', replicas only 'reads'
** Disaster recovery
No need to UPDATE connection string ( so the failover is seemless )
One DNS can be set to 'standby' 
Increases availability
Failover in case of disaster ( replication is SYNC )
No manual intervention
Not used for scaling (standby doesn't have read writes)
** Backups
Automatically enabled
Daily snapshot
Transaction logs saved for 7 days (35max)
DB snapshots can be user triggered ( retained for as long as you want)
** Security
*** Access
RDS DB's are deployed within private subnet ( not a public one )
Uses security groups ( to control who can COMMUNICATE with RDS )
Uses IAM policies to control who can MANAGE
Username / password for LOGIN to DB ( IAM users can be used - Postgre / Aurora)
*** Encription
Encription at REST with AWS KMS - AES-256 encription
You can use SSL certs to encrypt data to RDS in FLIGHT
*** Enforce SLL ! ( important )
**** PostgreSQL
rds.force_ssl=1 (in the AWS RDS Console - Parameter Groups)
**** MySQL
GRANT USAGE ON *.* TO 'mysqluser'@'%' REQUIRE SLL;
*** Connect using SLL
Provide SLL Trust cert ( can be downloaded from AWS )
Provide SLL options when connection to DB
*** Aurora
Compatibile with postre & mysql
'Cloud optimized'
Automatically grows in increments of 10GB up to 64TB
Can have up to 15 replicas
Replication is faster
Failover is instantaneous
HA native ( high availability native )
More expensive but also more efficient
* ElastiCache
In memory version of RDS ( Redis / Memcached )
Has Read & Write scaling ( Replicas / Sharding )
Multi AZ with failover
Can be encrypted at rest & in-transit
* VPC
Virtual Private Cloud
Each VPC contains subnets ( networks )
Each subnet must be mapped to AZ
Public & Private subnets CAN communicate if they are in the same VPC
** Public subnets ( available to public ):
Load balancers
Static websites
Files
Public authentication layers
** Private subnets 
Web app servers
Databases
* S3
Simple Storage Service
No directories ( just keys )
** Buckets
Defined @ regional level BUT must have UNIQUE name GLOBALLY !
*** Naming convention:
- No uppercase
- No underscore
- 3 to 63 chars
- not an IP
- starts with letter or number
** Objects
They have a key
*** Key
Its a FULL path to an Object(file)
<my_bucket>/my_file.txt
*** Values 
Content of the body of object
Max size is 5TB
If file > 5GB must use 'multi-part upload' !important
*** Metadata
List of text key / value pairs ( sys or user metadata )
*** Tags 
Useful for security / lifecycle
** Versioning
Enabled at bucket level
File before we turn on versioning will have version 'null' !important
Deleting versined file doesn't remove it, just puts 'delete marker' on it
** Encryption !important
4 Methods:
*** SSE-S3
Keys managed by AWS S3
Key name: "S3 Managed Data Key"
Objects encrypted server side
Encryption: AES-256
When sending data to S3: set header: "x-amz-server-side-encryption":"AES256"
*** SSE-KMS
Key Managed Service
Keys managed by Key Management Service
Key is called: "Customer Master Key" ( CMK )
KMS gives you more control over keys and audit trail
Server side encryption
When sending data to S3: set header: "x-amz-server-side-encryption":"aws:kms"
*** SSE-C
Keys managed by YOU outside of AWS ( not stored by amazon)
HTTPS must be used
How to:
Generate client side data key
Using Https send DATA key in header
Amazon encrypts object using key, and then discards the key
*** Client Side Encryption
Use client library such as Amazon S3 Encryption Client
Client must encrypt when sending & receiving to S3
Client fully manages keys & encryptin cycle
How to:
Generate data key
Encrypt object using key ( on client side )
Send encrypted object to S3 ( http/s )
*** Encryption in transit
Also called SSL / TLS
HTTP endpoint: not encrypted
HTTPS endpoint: encryption in flight ( mandatory for SSE-C )
** Security
*** User based
IAM policies - which API calls should be allowed for a specific user 
*** Resource based (more popular)
Bucket policies - bucket wide policy from S3 console ( allows cross account )
Use S3 policy to:
- Grant public access to bucket
- Force object to be encrypted on upload
- Grant access to another account ( Cross Account )
JSON based policies:
**** Resources: buckets and objects
**** Actions: Set of API to Allow or Deny
**** Effect: Allow / Deny
**** Principal: the account/user to apply policy to


*** Object Access Control List (ACL) !ignore
*** Bucket Access Control List !ignore
** Networking
S3 supports VPC Endpoints ( for instances in VPC without www internet )
** Logging and Audit
S3 access logs can be stored in other S3 bucket ( not the same - or you will have endless loop)
API calls can be logged in AWS CloudTrail
** User Security
MFA ( multi factor authentication ) can be enabled for file deletetion
Signed URLs: valid only for a limited time 
** S3 Websites
<bucket-name>.s3-website-<AWS-region>.amazonaws.com
<bucket-name>.s3-website.<AWS-region>.amazonaws.com
403 error --- > check bucket policy, does it allows public reads?!
** CORS
In order to share files from different bucket, that bucket needs to have a CORS enabled
and configured correctly
** Consistency Model
EVENTUAL CONSISTENCY !important
shit takes a while to update

for example, you try GET on resource and get 404 response
404 gets cached
you PUT the resource and try GET again
you will get 404 AGAIN because, 404 from first try was cached
it will take some time before resource will be available

Another example:
DELETE 200 -> GET 200
After deleting a resource, you might still be able to GET it for a short while
EVENTUAL CONSISTENCY RULE 
** Performance !important
*** If you have > 100 TPS performance might degrade
For best performance you want your objects distributed between different partitions
PUT 4 RANDOM CHARS in front of your KEY NAME to optimise performance !important

<my_bucket>/6ad7_myfolder/my_file1.txt
<my_bucket>/a37f_myfolder/my_file2.txt

(you don't have to do this anymore, but exam wasn't updated)

*** For files > 5GB use MULTIPART UPLOAD

*** If you want to do A LOT of reads, use CLOUDFRONT (caches S3 objects)

*** If you want to UPLOAD a lot, use S3 Transfer Acceleration
It uses edge locations

*** SSE-KMS Encryption can lower performance
You may have AWS limit for KMS usage ( 100s - 1000s download/upload per second)

* =========
* CLI
* =========

* Commands
//configure cli
aws configure  //afterwards put in both your keys in
//list buckets
aws s3 ls s3://bucketofwitold
//copy from bucket to bucket
aws s3 cp s3://bucketofwitold/dog.jpg s3://mysecondbucketwitold
//make bucket 
aws s3 mb s3://bucket-name-unique
//remove bucket
aws s3 rb s3://bucket-name-unique
//create t2 instance
aws ec2 run-instances --image-id ami-030dbca661d402413 --instance-type t2.micro
//test command flag ( will not run )
--dry-run
* EC2
NEVER PUT YOUR CREDENTIALS on EC2!!!
Use IAM Roles to give credentials to EC2
EC2 can have only 1 ROLE at a time
You can have as many roles as you want...just one can be used per given instance!
Each role can have multiple permissions
* EC2 Metadata
Information about ec2 instance
DONT FORGET TRAILING SLASH!
http://169.254.169.254/latest/meta-data/

//get temporary credentials
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/EC2_rolecurl http://169.254.169.254/latest/meta-daa/iam/security-credentials/EC2_role
* STS command line
Security Token Service
aws sts decode-authorization-message --encoded-message
* SDK
** default credentials provider chain
to authorize sdk  you can use:
- aws credentials on your local computer
- instance profile credentials using AIM Roles (on EC2 machine etc)
- environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
** Exponential backoff
Included in SDK
For rate limited api's
Each retry will take x2 of the previous delay before trying again
* =========
* ELASTIC BEANSTALK
Relies on CLOUD FORMATION under the hood
*** 3 Components
**** Application
**** Application version
**** Environment name (dev, test, prod)
*** Deployment modes
**** Single instance
Good for development
1 instance + security group + elastic IP + auto scaling group in one AZ 
**** High Availability with Load Balancer
Good for prod
Elastic load balancer talk to ASG ( auto scale group )
Multi instances auto scaled, across multi AZ, load balancer
*** Update modes
**** All at once
Fastest, 
instances will be temporarily out ( there will be downtime )
No cost
**** Rolling
Updates backet (N number of instances @ one time to be updated) at a time
Moves to next bucket when the updated one is healthy
Can take long time if many instances 
No cost
Runs below capacity
**** Rolling with additional batches
Spins up a batch to serve all content while the bucket is being updated ( no hit to performance )
Additional cost (extra instances spinned up to cover downed instances )
No downtime
Runs at capacity
**** Immutable
Spin up new instances running updated app, in new ASG.
Swap for old one when all is healthy
Longest deployment time 
Great for prod
Double capacity at a time
No downtime
High extra cost
**** Blue / Green
Deploy new environment
Spin up 100 % extra instances with new code
Redirect a bit of traffic using Route 53 to new environment ( weighted policy )
When all is ok
DNS changed when all is ok
Takes long time to deploy
*** Updating 
New code has to be ZIPped
All params can be set up within code
Files have to be in a folder called, in the ROOT level of the application
File format: YAML / JSON
Files must end with: .config extension
You can change default settings using OPTION_SETTINGS parameter
You can additional resources using those options
#+BEGIN_SRC 
.ebextensions/
#+END_SRC
IF YOU DELETE environment, you will loose all .ebextension resources associated with it ( also RBS )
*** Optimization
This is how the process looks like:
1) Describe dependencies ( eg. package.json )
2) Package zip with source code
3) Upload zip to EC2
4) Resolve / download dependencies on each EC2

Last step takes the longes ( if lots of deps )

TO INCREASE SPEED, PACKAGE DEPENDENCIES WITH SOURCE CODE
*** CLI
**** commands
eb deploy
eb create
eb status
eb health
eb open
...
**** helpful for automated deployment pipelines
* =========
* CI CD
** CodeCommit
Version control
Private Git repository
No size limit
Fully managed, highly available
Cody stays in AWS Cloud account ( security )
Secure ( encryption , access control etc )
Integreated with CI tools ( CodeBuild, Jenkins etc )
*** Requirements
You must have
- 1.7.9 github version to use CodeCommit
- IAM policy & user for accessing AWS CodeCommit OR belong to CODESTAR project team

** CodeCommit Security
Interaction used with Git commands
*** Authentication: ( you are who you say you are )
-HTTPS (use AWS CLI Authentication or generate HTTPS credentials )
-SSL Keys (configure keys via AIM Console)
-MFA ( multi factor )
*** Authorization: ( you have access to do stuff )
IAM Policies manage user / roles access to repos
*** Encryption
REST: Repos are encrypted automatically using KMS
TRANSIT: https or ssl
*** Cross Account Access
Use IAM Role and AWS STS ( AssumeRole API )
DO NOT SHARE SSH / AWS credentials EVER!
*** CodeCommit vs GitHub
**** Similar
Both are git repos
Support code reviews ( Pull Requests )
Can be integrated with CodeBuild
Support HTTPS and SSH for authentication
**** Differences
***** Security
Github is administered using GITHUB USERS
CodeCommit uses IAM roles & users
***** Hosted
GitHub is 3rd party
CodeCommit is manages & hosted by AWS
***** Notifications
: every time someone commits code, we can trigger events / background checks
: for example, lambda can check if there are any credentials in the code ...
CodeCommit can be integrated with
- AWS SNS ( Simple Notification Service )
- AWS Lambda
- AWS CloudWatch Event Rules
*** Notification 
Use cases:
**** SNS / Lambda
1) Push to existing branch
2) Create branch
3) Delete branch
4) Trigger Lambda OR SNS
**** CloudWatch Rules
1) Trigger for Pull Requests ( create / update / delete / comment )
2) Commit comments events
3) CloudWatch Event Rules goes into an SNS topic ( sns - simple notification service)
4) CloudWatch will send email notification
* CodePipeline
Each stage generates 'artifacts' that are stored in S3 bucket ( and passed from there to next stage )
Each change in state generates CloudWatch Events which in turn can create SNS notification ( you can create events for failed pipelines )
On failure at any stage, pipeline stops
You can use AWS CloudTrail to audit API calls
If pipeline fails stage check if it has necessary permissions "IAM Service Role" (check attached policies)
When created CodePipeline creates CloudWatch Event rule that will trigger ( on commit or can check periodically )
* CodeBuild
Fully managed build service
Continuous scaling ( no build queue,build server to manage etc ...  unlike Jenkins )
Pay for usage ( the time used to build )
Uses docker ( you can use your own docker images )
Build instructions CAN be defined in code ( in BUILDSPEC.YML file) !important
Output logs ===> S3 & CloudWatch
Use CloudWatch to trigger notifications ( and detect failed builds etc)
Use Lambda as Glue
Use SNS notifications
You can reproduce build locally to troubleshoot
You can use docker to extend any environment you like ( eg. Huskel ) - its fully extensible because of it
You can use S3 Cache Bucket to cache build dependencies ( if you say are doing multiple builds etc )
** When build finishes / fails ===> CONTAINER GETS DELETED!
** Code source
Github
CodeCommit
CodePipeline
S3
...
** Integrations
KMS for encription
IAM for build permissions
VPC for network security
CloudTrail for API calls tracing
** buildspec.yml
Must be at the ROOT of the code
Contains variables ( plaintext or SSM param store )
*** Phases
Install : gets dependencies
Pre-build : final commands before build
Build : actual build commands
Post build : finishing touches ( zipping files )
Upload artifacts to S3
Cache files to S3 ( usually dependencies )
*** Local build ( for troubleshooting )
Uses CodeBuild Agent
* CodeDeploy
Each EC2 ( or On Premise machine ) MUST run CodeDeploy Agent
Agent keeps asking CodeDeploy for work
CodeDeploy points at APPSPEC.YML !important
Application is pulled from GitHub / S3
EC2 runs deployment instructions
CodeDeploy reports success / failure of the deployment

EC2 instances are grouped by deployment group ( dev / qa / prod )
CodeDeploy can be chained into CodePipeline and use artifacts from there 
CodeDeploy can reuse tools , autoscaling integrations, other apps
Blue / Green works with EC2 instances
CodeDeploy supports AWS Lambdas
CodeDeploy DOES NOT provision resources ( EC2 instances are assumed to already exist! )
** appspec.yml !important
IN ROOT FOLDER !
*** file section
how to source & copy files from S3 / GitHub
*** hooks ( can have timeouts )
Not all of them have to be defined 
deployment instructions
Order of hooks:
1) Application stop : Stop application

2) DownloadBundle : How to download app

3) BeforeInstall : Prep before instal

4) AfterInstall : cleaup etc

5) ApplicationStart : start up

6) ValidateService : check if deployment succeded
*** deplyment config
One at a time
Half at a time
All at once ( quickest but with downtime - good for dev )
Custom ( eg 75% )
*** failure
If deploy fails, instance STAYS in fail state
New deploy starts with failed instances
To rollback : redeploy previous version OR enable automatic rollback for failures
*** deployemnt target 
EC2 instances with tags
AutoScalingGroup (ASG)
Mix of above
DEPLOYMENT_GROUP_NAME variable for extra config
*** permissions that have to be set up
2 roles:
- For CodeDeploy ( AWS CodeDeploy policy )
- For EC2 ( access to S3 -- read only )
*** HOW TO SET UP DEPLOYMENT
1) Make sure you have EC2 instances / In place instances
2) Give IAM role to EC2 instances ( to access data where source code will be )
3) Install DeploymentAgent on EC2 instances
4) Put a tag on EC2 instances

5) Give role CodeDeploy
6) Set up CodeDeploy groups
7) Make sure APPSPEC.YML is defined in ROOT of the source code
** Cloud Formation
Infrastructure as Code ( you declare what you want, and AWS gets it for you )
Can be version controlled
Changes to infrastructure are done via code review
It's FREE
Easy to estimate cost of infrastructure created via Cloud Formation
Helps to save ( destroy infrastrucutre in the evening and recreate it in the morning )
Declarative programming ( Cloud Formation figures out the order of things and orchestration )
Makes SEPARATION OF CONCERNS easy ( you can have separate stack for VPC , network, App stack ...)
*** Templates
- Have to be uploaded to S3
- You can't edit them, upload NEW version, CF will figure out the difference between them
- Stacks are identified by name
- Deleting a stack will delete every associated artifact created by CF
**** Deploying templates
***** Manual
- Edit template in CloudFormation Designer
- Use console to insert parameters
***** Automated
- Edit template in YAML file
- Use CLI to deploy
**** Template elements
1) Resources ( eg. EC2, LoadBalancers, Security Groups....) //MANDATORY
2) Parameters (dynamic inputs)
3) Mappings (static variables)
4) Outputs ( other CF can reference those )
5) Conditionals ( if statements that controll what gets created )
6) Metadata
**** Template Helpers
1) References
2) Functions
**** Template: Resources
***** Mandatory part of template
***** Represent componenets that will be created / configured
***** Can reference each other
***** Over 224 types of resources
***** AWS::aws-product-name::data-type-name
***** YOU CANNOT CREATE DYNAMIC AMOUNT OF RESOURCES!
***** Almost every service is supported by CloudFormation
**** Template: Parameters
Use if template configuration might change in future
If parameter changes, you don't have to re-upload the whole template
Parameters can be referenced anywhere in your template
#+BEGIN_SRC 
Fn::Ref      ( short version: )   !Ref
#+END_SRC
***** Pseudo Parameters
Eg:
AWS::AccountId
**** Template: Mappings
Fixed variables in CloudFormation Template
Values are hardcoded in the template
Eg: region map with ami codes for each zone
***** Access mappings values
Fn::FindInMap   || !FindInMap (shorthand version)
**** Template: Outputs
Optional values being exported from the template
Outputs can be viewed in AWS Console or CLI
Used for cross-stack collaboration ( each expert looks after their domain and outputs needed details used in other stacks)
You CANNOT DELETE stack if it's output is being referenced in some other stack!
Exporting value is OPTIONAL
**** Template: Imports
Fn::ImportValue     !ImportValue (shorthand)
**** Template: Outputs
Based on condition Resources can be created || Values output
Conditions are on the same level as a resource TYPE that is to be created should condition evaluate to TRUE
#+BEGIN_SRC 
Resources:
  MountPoint:
    Type: "AWS::EC2::VolumeAttachment"
    Condition: SomeConditionName
#+END_SRC
types of conditions: environment, region, parameter value... anything you want
Conditions can referece other conditions
Eg:
#+BEGIN_SRC javascript
  Condtions:
     CreateProd: !Equals [ !Ref EnvType, prod ]
//compares the two values in brackets
#+END_SRC
*** Intrinsic Functions
**** Fn::Ref !important
References parameter / resource 
Returns VALUE of param / ID of resource !
**** Fn::GetAtt
Gets RESOURCES attribute
#+BEGIN_SRC 
!GetAtt ResourceName.attributeName
#+END_SRC
**** Fn::FindInMap
Get a value from specific key
!FindInMap [ MapName, TopLevelName, SecondLevelName ]
**** Fn::ImportValue
Import value of the param that has been EXPORTED
**** Fn::Join
Join a list of values with a specified delimiter
#+BEGIN_SRC 
!Join [ delimiter, [comma-delimited list of values ] ]
!Join [ : , [a,b,c]] ---> a:b:c
#+END_SRC
**** Fn::Sub
Substitution
#+BEGIN_SRC 
!Sub 'arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:vpc/${vpc}'
#+END_SRC

#+BEGIN_SRC 
Name: !Sub
  - www.${Domain}
  - { Domain: !Ref RootDomainName }
#+END_SRC
**** Conditions ( Fn::If, Fn::Else, Fn::Not, Fn::Equals ...)
*** Rollbacks
**** Creation fail
If stack creation fails: everything is rolled back ( default ) ie. everything gets deleted
You can disable rollback 
**** Update fail
Rolls back to last working state
* ==========
* Monitoring
* ==========
* CloudWatch
Provides monitoring for EVERY service in AWS
Metric is a VARIABLE to monitor ( cpu utilization etc)
Metrics belong to namespaces
Metrics have timestamps
** Metric: dimension
Dimension is an attribute of Metric ( eg: instance id )
You can have up to 10 dimensions per metric
** EC2
Monitoring every 5 min
For extra pay you can monitor every 1 min ( Detailed monitoring )
Memory usage is not 'pushed' by default, must be pushed from instance as a custom metric
** Metric: resolution
FOR CUSTOM METRICS:
How often we get data
By default: every 1 minute
Detailed resolution : up to [ every 1 second ]
** PutMetricData
Use this API call to send the metric data
** Use exponential back off
** Alarms
Can be triggered by any metric of our choice
Can go to ASG, EC2, SNS notifications
Alarm can be triggered up to every 10s ( MAX ) ( on high resolution metric )
*** Alarm States
- OK
- INSUFFICIENT_DATA
- ALARM
** Logs
By default: logs NEVER EXPIRE
Applications can send LOGs to CloudWatch ( using SDK )
Can be collected from : BeansTalk, ECS, VPC flow logs, API Gateways, CloudTrail, Route53, CloudWatch Log Agents on EC2 machines, Lambdas..
Can be passed on to S3 for archiving
Can be STREAMED to ElasticSearch for further analytics or to AWS LAMBDA
Can use filter expressions
Can define expiration policy ( never, 30 days ...)
YOU HAVE TO SET UP IAM PERMISSION to send logs to CloudWatch!
*** Log storage architecture
Log groups: arbitrary name
Log stream: instances within application / log files / containers
*** Security
Logs can be encrypted using KMS at GROUP LEVEL
** X-Ray
*** Helps:
- Troubleshoot performance, finds bottlenecks
- Understand dependencies in our architecture ( visual )
- Pinpoints service issues
- Find errors and exceptions
- Find if we meet our SLA obligations etc
- Identify users that are impacted
*** Compatible with:
- Lambda
- Beanstalk
- ECS
- ELB
- API Gateway
- EC2 instances
- On Premise servers
*** Tracing
End to end following of request
Each component dealing with request adds its own 'trace'
Trace is made out of SEGMENTS
Gives you ability to trace every request
*** Security
KMS at rest
IAM for authorization
*** How to enable !important
Import in code base, then install daemon which will send 1s batches to AWS X-Ray

**** Code: Java, Go, .Net, Nodejs, Python MUST import AWS SDK
Application SDK will then capture:
- calls to AWS services
- HTTP/s requests
- Database calls
- Queue calls
**** Install X-Ray daemon || Enable X-Ray AWS Integration
Works as a low level UDP packet interceptor
Each application must have IAM right to write to X-RAY
**** MY X-RAY ISNT WORKING!?
- check IAM permissions on EC2 ( or whatever )
- check if X-Ray DAEMON is installed & running on your EC2
**** X-Ray on Lambda
- enable IAM permission for Lambda ( AWSX-RayWriteOnlyAccess )
- ensure X-Ray is imported in code
** CloudTrail
Tracks User activity & API usage
Governance, audit, compliance for AWS Account
Enabled by default
History of events / API calls made out from my AWS account: 
- Console
- CLI
- SDK
- AWS Services
You can put output to CloudWatch Logs
If something gets deleted....check CloudTrail first !important
* Messaging
Syncronous
Asyncronous
Below services can scale independently from the application
** SQS !important
Simple Queue Service
PRODUCES send messages to SQS ( 1 to many )
CONSUMERS poll SQS for messages ( 1 to many )
Producers and Consumers are independent from each other, and can scale 
*** Standard Queue
Fully managed
From 1 to 10,000 messages a second
DEFAULT message retention: 4 days ( up to 14 days )
No limit on number of messages
Low latency ( < 10ms )
Horizontal scaling in terms of CONSUMERS
CAN HAVE DUPLICATE MESSAGES
BEST EFFORT ORDERING ( messages CAN be out of order !)
Max message size 256KB
** SNS
pub/sub model
** Kinesis
real time streaming model
