
;; Fundamentals
* Need to know
** EC2
*** how to ssh into ec2 
*** how to change .pem permissions (chmod)
gives a permission error exception if you can't access .pem
*** how to use security groups
*** differences between private/public/elastic IP
*** how to use User Data at boot time
*** know that you can use custom AMI
*** instances are billed per second
* Linux commands
** install
yum install
** switch to root
sudo su
* Regions
** All regions are scoped
Except for IAM and s3
* AMI
Amazon Machine Image
They are region locked, cannot be seen from outside a region
* IAM
Identity and Access Management
It's global ( not locked to a zone / region )
* SECURITY
Never use root account ( except when first setting up )
*** Users
Physical person
*** Groups
Functions ( admins )
*** Roles
For machines
*** Policies
Govern permissions
Define what each above groups can do
*** Leas privilege principle 
Give minimal permissions to users they need to get job done
*** Permissions
**** 0644 <key_name> are too open
chmod 0400 <key_name> (changes permissions to -r-----) (only owner can read)
*** Security groups
Act like firewall and block all Inboud / Outbound traffic that is not greenlit
Can be attached to multiple instances ( vm's )
Instance can have mutliple groups
They work only within their region ( if you switch region, you have to recreate groups )
They 'live' outside EC2 - they don't run on instances
Defaults: all inbound is blocked and all outbound is allowed
You can reference other security groups / CIDR blocks / IP addresses
You CANNOT reference DNS names 
* EC2
Elastic compute cloud
** Cannot see IP of customer going through load balancer
Can only see private IP of ALB, in order to get IP of client
check the header 'X-Forwarded-For'
to check the port check 'X-Forwarded-Port'
** Custom AMI
Customers can spin up their own versions of OS
with custom setup/software/access etc
** T2 Instances 
They are burstable :
they have 'cpu credits', and under unexpected load
they can perform very well, unless they run out of cpu creds
** User Data
Used to run commands when the instance starts up
Commands run with 'sudo'
Every bash script has to start with
#+BEGIN_CENTER bash
   #!/bin/bash
#+END_CENTER
** shh into machine
ssh -i <key-file.pem> ec2-user@ip-address
** shh on windows
use PUTTY to ssh into it
** Launch Types
*** On demand
short workload, predictable pricing
*** Reserved Instances
long workloads ( 1+ year )
*** Convertible Reserved Instances
long workloads with flexi instances
*** Scheduled Reserved Instances
launch within reserved time window
*** Spot Instances
short workloads, cheap, can loose instances!
*** Dedicated Instances
no other customer will share hardware
*** Dedicated Hosts
book entire server
** Pricing
Depends on:
-region
-instance type
-on demand / reserved / dedicated / spot
-type of OS
-billed per second, with min of 60
* Apache
** install apache
yum install httpd.x86_64
(http daemon)
** run & enable apache
systemctl start httpd.service
systemctl enable httpd.service
* Load Balancers
** Can scale but need to 'warm up'
** CLB/ALB/NLB has a static host name 
Do not resolve and use underlying IP
** Provide SSL certs & SSL termination (Classic and ALB)
** Types
v1 balancer ( classic )
v2 application / network balancer
** Health checks ( available to all ALB )
ELB can perform health check on instance
Done on PORT & ROUTE ( /health )
** Application Load Balancer (v2)
Layer 7 
Can route based on hostname / path
Great fit with ECS (Docker)
Can handle/serve multiple groups/apps
Can implement 'stickiness' and direct same user to the same group ( ALB will generate cookie, not your application !)
Supports HTTP/HTTPS & Websockets protocols
APPLICATIONS dont see IP of clients directly! ( Ip is placed in header 'X-Forwarded-For)
** Network Load Balancer (v2)
Layer 4
Forwards TCP traffic
** CLB/ALB cannot see client IP directly
NLB can see IP directly
Ip is stored in 'X-Forwarded-For' header (for alb/clb)
** 4xx errors
Client induced error
** 5xx errors
Application/Server side errors
** If unable to connect to application..
Check security groups!
* ASG
Auto Scaling Groups
Free!
Can scale based on CloudWatch alarms
Its possible to set up custom metrics for CloudWatch
ASG use launch configurations 
IAM roles attached to ASG are PASSED on to EC2 instances 
** Metrics
CPU
Network
Custom
On schedule!
* EBS Volume
Network drives
Only one EBS can be attached/detached to EC2's
Locked to Availability Zone eg: us-east-1a != us-east-1b
You can move SNAPSHOTS of it
EBS backups use IO so don't perform them when application is busy
Root EBS gets terminated with the instance ( can be turned off )
Get charged by provisioned size, not used one
You can resize volumes
** EBS encryption !important
Data at rest is encrypted inside the volume
Data moving between instance & volume is encrypted
Snapshots created from this volume are encrypted
** Snapshots
Snapshots take actual size not the whole provision of EBS 
Used for backups
When you want to resize a volume down
Change volume type
Encrypt volume
** Instance stores
Physically attached to the machine
Better I/O performance
On termination of instance, data is lost
Cant resize
Backups must be operated by user
* Route 53
Use Alias over CNAME
Managed DNS ( collection of records )
DNS records get cached by browsers ( saved locally )
** Records:
*** A: 
url to ipv4
*** AAAA:
url to ipv6
*** CNAME
url to url
*** Alias
url to AWS resource
** Features:
Load balancing
Health checks
Routing policy ( geolocation, proximity, latency, weighted, simple)
* RDS
Relational Database Services
** Replicas
Applications MUST update the connection string to leverage read replicas!!
Used for READ scaling !
DB can have up to 5 replicas within & across AZ or regions
Replication is ASYNC !
Replicas can be promoted to their own DB
Master has 'write/read', replicas only 'reads'
** Disaster recovery
No need to UPDATE connection string ( so the failover is seemless )
One DNS can be set to 'standby' 
Increases availability
Failover in case of disaster ( replication is SYNC )
No manual intervention
Not used for scaling (standby doesn't have read writes)
** Backups
Automatically enabled
Daily snapshot
Transaction logs saved for 7 days (35max)
DB snapshots can be user triggered ( retained for as long as you want)
** Security
*** Access
RDS DB's are deployed within private subnet ( not a public one )
Uses security groups ( to control who can COMMUNICATE with RDS )
Uses IAM policies to control who can MANAGE
Username / password for LOGIN to DB ( IAM users can be used - Postgre / Aurora)
*** Encription
Encription at REST with AWS KMS - AES-256 encription
You can use SSL certs to encrypt data to RDS in FLIGHT
*** Enforce SLL ! ( important )
**** PostgreSQL
rds.force_ssl=1 (in the AWS RDS Console - Parameter Groups)
**** MySQL
GRANT USAGE ON *.* TO 'mysqluser'@'%' REQUIRE SLL;
*** Connect using SLL
Provide SLL Trust cert ( can be downloaded from AWS )
Provide SLL options when connection to DB
*** Aurora
Compatibile with postre & mysql
'Cloud optimized'
Automatically grows in increments of 10GB up to 64TB
Can have up to 15 replicas
Replication is faster
Failover is instantaneous
HA native ( high availability native )
More expensive but also more efficient
* ElastiCache
In memory version of RDS ( Redis / Memcached )
Has Read & Write scaling ( Replicas / Sharding )
Multi AZ with failover
Can be encrypted at rest & in-transit
* VPC
Virtual Private Cloud
Each VPC contains subnets ( networks )
Each subnet must be mapped to AZ
Public & Private subnets CAN communicate if they are in the same VPC
** Public subnets ( available to public ):
Load balancers
Static websites
Files
Public authentication layers
** Private subnets 
Web app servers
Databases
* S3
Simple Storage Service
No directories ( just keys )
** Buckets
Defined @ regional level BUT must have UNIQUE name GLOBALLY !
*** Naming convention:
- No uppercase
- No underscore
- 3 to 63 chars
- not an IP
- starts with letter or number
** Objects
They have a key
*** Key
Its a FULL path to an Object(file)
<my_bucket>/my_file.txt
*** Values 
Content of the body of object
Max size is 5TB
If file > 5GB must use 'multi-part upload' !important
*** Metadata
List of text key / value pairs ( sys or user metadata )
*** Tags 
Useful for security / lifecycle
** Versioning
Enabled at bucket level
File before we turn on versioning will have version 'null' !important
Deleting versined file doesn't remove it, just puts 'delete marker' on it
** Encryption !important
4 Methods:
*** SSE-S3
Keys managed by AWS S3
Key name: "S3 Managed Data Key"
Objects encrypted server side
Encryption: AES-256
When sending data to S3: set header: "x-amz-server-side-encryption":"AES256"
*** SSE-KMS
Key Managed Service
Keys managed by Key Management Service
Key is called: "Customer Master Key" ( CMK )
KMS gives you more control over keys and audit trail
Server side encryption
When sending data to S3: set header: "x-amz-server-side-encryption":"aws:kms"
*** SSE-C
Keys managed by YOU outside of AWS ( not stored by amazon)
HTTPS must be used
How to:
Generate client side data key
Using Https send DATA key in header
Amazon encrypts object using key, and then discards the key
*** Client Side Encryption
Use client library such as Amazon S3 Encryption Client
Client must encrypt when sending & receiving to S3
Client fully manages keys & encryptin cycle
How to:
Generate data key
Encrypt object using key ( on client side )
Send encrypted object to S3 ( http/s )
*** Encryption in transit
Also called SSL / TLS
HTTP endpoint: not encrypted
HTTPS endpoint: encryption in flight ( mandatory for SSE-C )
** Security
*** User based
IAM policies - which API calls should be allowed for a specific user 
*** Resource based (more popular)
Bucket policies - bucket wide policy from S3 console ( allows cross account )
Use S3 policy to:
- Grant public access to bucket
- Force object to be encrypted on upload
- Grant access to another account ( Cross Account )
JSON based policies:
**** Resources: buckets and objects
**** Actions: Set of API to Allow or Deny
**** Effect: Allow / Deny
**** Principal: the account/user to apply policy to


*** Object Access Control List (ACL) !ignore
*** Bucket Access Control List !ignore
** Networking
S3 supports VPC Endpoints ( for instances in VPC without www internet )
** Logging and Audit
S3 access logs can be stored in other S3 bucket ( not the same - or you will have endless loop)
API calls can be logged in AWS CloudTrail
** User Security
MFA ( multi factor authentication ) can be enabled for file deletetion
Signed URLs: valid only for a limited time 
** S3 Websites
<bucket-name>.s3-website-<AWS-region>.amazonaws.com
<bucket-name>.s3-website.<AWS-region>.amazonaws.com
403 error --- > check bucket policy, does it allows public reads?!
** CORS
In order to share files from different bucket, that bucket needs to have a CORS enabled
and configured correctly
** Consistency Model
EVENTUAL CONSISTENCY !important
shit takes a while to update

for example, you try GET on resource and get 404 response
404 gets cached
you PUT the resource and try GET again
you will get 404 AGAIN because, 404 from first try was cached
it will take some time before resource will be available

Another example:
DELETE 200 -> GET 200
After deleting a resource, you might still be able to GET it for a short while
EVENTUAL CONSISTENCY RULE 
** Performance !important
*** If you have > 100 TPS performance might degrade
For best performance you want your objects distributed between different partitions
PUT 4 RANDOM CHARS in front of your KEY NAME to optimise performance !important

<my_bucket>/6ad7_myfolder/my_file1.txt
<my_bucket>/a37f_myfolder/my_file2.txt

(you don't have to do this anymore, but exam wasn't updated)

*** For files > 5GB use MULTIPART UPLOAD

*** If you want to do A LOT of reads, use CLOUDFRONT (caches S3 objects)

*** If you want to UPLOAD a lot, use S3 Transfer Acceleration
It uses edge locations

*** SSE-KMS Encryption can lower performance
You may have AWS limit for KMS usage ( 100s - 1000s download/upload per second)

* =========
* CLI
* =========

* Commands
//configure cli
aws configure  //afterwards put in both your keys in
//list buckets
aws s3 ls s3://bucketofwitold
//copy from bucket to bucket
aws s3 cp s3://bucketofwitold/dog.jpg s3://mysecondbucketwitold
//make bucket 
aws s3 mb s3://bucket-name-unique
//remove bucket
aws s3 rb s3://bucket-name-unique
//create t2 instance
aws ec2 run-instances --image-id ami-030dbca661d402413 --instance-type t2.micro
//test command flag ( will not run )
--dry-run
* EC2
NEVER PUT YOUR CREDENTIALS on EC2!!!
Use IAM Roles to give credentials to EC2
EC2 can have only 1 ROLE at a time
You can have as many roles as you want...just one can be used per given instance!
Each role can have multiple permissions
* EC2 Metadata
Information about ec2 instance
DONT FORGET TRAILING SLASH!
http://169.254.169.254/latest/meta-data/

//get temporary credentials
curl http://169.254.169.254/latest/meta-data/iam/security-credentials/EC2_rolecurl http://169.254.169.254/latest/meta-daa/iam/security-credentials/EC2_role
* STS command line
Security Token Service
aws sts decode-authorization-message --encoded-message
* SDK
** default credentials provider chain
to authorize sdk  you can use:
- aws credentials on your local computer
- instance profile credentials using AIM Roles (on EC2 machine etc)
- environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
** Exponential backoff
Included in SDK
For rate limited api's
Each retry will take x2 of the previous delay before trying again
* =========
* ELASTIC BEANSTALK
Relies on CLOUD FORMATION under the hood
*** 3 Components
**** Application
**** Application version
**** Environment name (dev, test, prod)
*** Deployment modes
**** Single instance
Good for development
1 instance + security group + elastic IP + auto scaling group in one AZ 
**** High Availability with Load Balancer
Good for prod
Elastic load balancer talk to ASG ( auto scale group )
Multi instances auto scaled, across multi AZ, load balancer
*** Update modes
**** All at once
Fastest, 
instances will be temporarily out ( there will be downtime )
No cost
**** Rolling
Updates backet (N number of instances @ one time to be updated) at a time
Moves to next bucket when the updated one is healthy
Can take long time if many instances 
No cost
Runs below capacity
**** Rolling with additional batches
Spins up a batch to serve all content while the bucket is being updated ( no hit to performance )
Additional cost (extra instances spinned up to cover downed instances )
No downtime
Runs at capacity
**** Immutable
Spin up new instances running updated app, in new ASG.
Swap for old one when all is healthy
Longest deployment time 
Great for prod
Double capacity at a time
No downtime
High extra cost
**** Blue / Green
Deploy new environment
Redirect a bit of traffic using Route 53 to new environment ( weighted policy )
When all is ok
DNS changed when all is ok
Takes long time to deploy
*** Updating 
New code has to be ZIPped
All params can be set up within code
Files have to be in a folder called, in the ROOT level of the application
File format: YAML / JSON
Files must end with: .config extension
You can change default settings using OPTION_SETTINGS parameter
You can additional resources using those options
#+BEGIN_SRC 
.ebextensions/
#+END_SRC
IF YOU DELETE environment, you will loose all .ebextension resources associated with it ( also RBS )
*** Optimization
This is how the process looks like:
1) Describe dependencies ( eg. package.json )
2) Package zip with source code
3) Upload zip to EC2
4) Resolve / download dependencies on each EC2

Last step takes the longes ( if lots of deps )

TO INCREASE SPEED, PACKAGE DEPENDENCIES WITH SOURCE CODE
*** CLI
**** commands
eb deploy
eb create
eb status
eb health
eb open
...
**** helpful for automated deployment pipelines
* =========
* CI CD
** CodeCommit
Version control
Private Git repository
No size limit
Fully managed, highly available
Cody stays in AWS Cloud account ( security )
Secure ( encryption , access control etc )
Integreated with CI tools ( CodeBuild, Jenkins etc )
*** Requirements
You must have
- 1.7.9 github version to use CodeCommit
- IAM policy & user for accessing AWS CodeCommit OR belong to CODESTAR project team

** CodeCommit Security
Interaction used with Git commands
*** Authentication: ( you are who you say you are )
-HTTPS (use AWS CLI Authentication or generate HTTPS credentials )
-SSL Keys (configure keys via AIM Console)
-MFA ( multi factor )
*** Authorization: ( you have access to do stuff )
IAM Policies manage user / roles access to repos
*** Encryption
REST: Repos are encrypted automatically using KMS
TRANSIT: https or ssl
*** Cross Account Access
Use IAM Role and AWS STS ( AssumeRole API )
DO NOT SHARE SSH / AWS credentials EVER!
*** CodeCommit vs GitHub
**** Similar
Both are git repos
Support code reviews ( Pull Requests )
Can be integrated with CodeBuild
Support HTTPS and SSH for authentication
**** Differences
***** Security
Github is administered using GITHUB USERS
CodeCommit uses IAM roles & users
***** Hosted
GitHub is 3rd party
CodeCommit is manages & hosted by AWS
***** Notifications
: every time someone commits code, we can trigger events / background checks
: for example, lambda can check if there are any credentials in the code ...
CodeCommit can be integrated with
- AWS SNS ( Simple Notification Service )
- AWS Lambda
- AWS CloudWatch Event Rules
*** Notification 
Use cases:
**** SNS / Lambda
1) Push to existing branch
2) Create branch
3) Delete branch
4) Trigger Lambda OR SNS
**** CloudWatch Rules
1) Trigger for Pull Requests ( create / update / delete / comment )
2) Commit comments events
3) CloudWatch Event Rules goes into an SNS topic ( sns - simple notification service)
4) CloudWatch will send email notification
* CodePipeline
Each stage generates 'artifacts' that are stored in S3 bucket ( and passed from there to next stage )
Each change in state generates CloudWatch event which in turn can create SNS notification ( you can create events for failed pipelines )
